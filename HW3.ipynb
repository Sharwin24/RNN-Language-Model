{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: RNN Language Model\n",
    "\n",
    "## 1A. Description and Illustration of RNN Language Model architecture and design choices\n",
    "\n",
    "The RNN language model architecture is designed to process sequential data, such as text, and predict the next token in a sequence. The model is implemented using PyTorch and consists of several key components. The `HyperParams` dataclass encapsulates the hyperparameters, including `vocab_size`, `batch_size`, `seq_length`, `learning_rate`, `num_epochs`, `hidden_dim`, `num_layers`, `embedding_dim`, and `dropout`. These parameters are used to configure the model's behavior and structure. The `RNN` class inherits from `nn.Module` and defines the model's architecture. It includes an embedding layer (`nn.Embedding`) to convert input tokens into dense vectors of size `embedding_dim`, followed by an RNN layer (`nn.RNN`) with `hidden_dim` units and `num_layers` layers. Dropout (`nn.Dropout`) is applied to the RNN output to prevent overfitting. Finally, a fully connected layer (`nn.Linear`) maps the RNN's hidden state to the output vocabulary space. The `forward` method processes the input sequence, computes embeddings, passes them through the RNN, applies dropout, and generates logits for the next token prediction. The model also maintains a hidden state, which is updated at each time step to capture sequential dependencies.\n",
    "\n",
    "## 1B. Model Hyperparameters\n",
    "\n",
    "```python\n",
    "single_layer_rnn = HyperParams(\n",
    "    vocab_size=10000, # Vocab size for unique tokens in the dataset\n",
    "    batch_size=64, # Batch size for training, adjusted based on GPU resources\n",
    "    seq_length=30, # Sequence length for truncated backpropagation through time\n",
    "    learning_rate=0.0005, # Learning rate found with emperical tuning\n",
    "    num_epochs=20, # Number of training epochs\n",
    "    hidden_dim=256,  # Dimension of the RNN hidden state\n",
    "    num_layers=1, # Number of RNN layers\n",
    "    embedding_dim=100, # Embedding dimension for input tokens\n",
    "    dropout=0 # No dropout for a single layer RNN\n",
    ")\n",
    "```\n",
    "\n",
    "## 1C. Learning Curves\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <img src=\"Experiment 0/loss_curve.png\" alt=\"Loss Curve\" style=\"width: 45%; border-radius: 15px;\">\n",
    "    <img src=\"Experiment 0/perplexity_curve.png\" alt=\"Perplexity Curve\" style=\"width: 45%; border-radius: 15px;\">\n",
    "</div>\n",
    "\n",
    "## 1D. Final Test Set Perplexity\n",
    "\n",
    "## 2. Improving the RNN language model architecture\n",
    "\n",
    "To improve upon vanilla RNNs wer can use advanced RNN variants, attention mechanisms, and regularization. Some examples of these are below:\n",
    "\n",
    "- LSTM (Long Short-Term Memory): Replace the vanilla RNN with LSTM cells, which are better at capturing long-term dependencies due to their gating mechanisms (input, forget, and output gates).\n",
    "\n",
    "- GRU (Gated Recurrent Unit): Use GRUs, which are a simpler alternative to LSTMs but still effective at handling long-term dependencies with fewer parameters.\n",
    "\n",
    "- Bidirectional RNNs: Implement bidirectional RNNs to capture context from both past and future tokens, which is particularly useful for tasks like text generation or sentiment analysis.\n",
    "\n",
    "- Self-Attention: Incorporate self-attention mechanisms (as used in Transformers) to allow the model to focus on relevant parts of the input sequence, improving its ability to handle long-range dependencies.\n",
    "\n",
    "- Layer Normalization: Apply layer normalization to stabilize training and improve convergence.\n",
    "\n",
    "- Learning Rate Scheduling: Use learning rate schedulers (e.g., cosine annealing or step decay) to adaptively adjust the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    '''Dataclass to store hyperparameters for the RNN Language Model'''\n",
    "\n",
    "    vocab_size: int\n",
    "    batch_size: int\n",
    "    seq_length: int\n",
    "    learning_rate: float\n",
    "    num_epochs: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    embedding_dim: int\n",
    "    dropout: float\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (\n",
    "                self.vocab_size,\n",
    "                self.batch_size,\n",
    "                self.seq_length,\n",
    "                self.learning_rate,\n",
    "                self.num_epochs,\n",
    "                self.hidden_dim,\n",
    "                self.num_layers,\n",
    "                self.embedding_dim,\n",
    "                self.dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __eq__(self, value: 'HyperParams'):\n",
    "        return (\n",
    "            self.vocab_size == value.vocab_size\n",
    "            and self.batch_size == value.batch_size\n",
    "            and self.seq_length == value.seq_length\n",
    "            and self.learning_rate == value.learning_rate\n",
    "            and self.num_epochs == value.num_epochs\n",
    "            and self.hidden_dim == value.hidden_dim\n",
    "            and self.num_layers == value.num_layers\n",
    "            and self.embedding_dim == value.embedding_dim\n",
    "            and self.dropout == value.dropout\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'HP(vocab_size={self.vocab_size}, batch_size={self.batch_size}, seq_len={self.seq_length}, lr={self.learning_rate}, epochs={self.num_epochs}, hl_dim={self.hidden_dim}, num_layers={self.num_layers}, emb_dim={self.embedding_dim}, do={self.dropout})'\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hp: HyperParams):\n",
    "        super().__init__()  # Remove 'self' from super() call\n",
    "        self.HP = hp\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(hp.vocab_size, hp.embedding_dim)\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=hp.embedding_dim,\n",
    "            hidden_size=hp.hidden_dim,\n",
    "            num_layers=hp.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=hp.dropout\n",
    "        )\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(hp.dropout)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hp.hidden_dim, hp.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass through RNN\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        # Take the last time step and pass through final layer\n",
    "        logits = self.dropout(rnn_out)\n",
    "        output = self.fc(logits)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN LLM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLLM:\n",
    "    def __init__(self, train_valid_test_files: tuple[str, str, str], hp: HyperParams):\n",
    "        self.train_file, self.valid_file, self.test_file = train_valid_test_files\n",
    "        self.HP = hp\n",
    "        os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "        os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using Torch device: {self.device}')\n",
    "\n",
    "    def train_models(self, hyperparams: list[HyperParams]):\n",
    "        hp_to_loss: dict[HyperParams, tuple[float, float, float]] = {}\n",
    "        # Each value in the dict is a tuple of (valid loss, test_loss, perplexity)\n",
    "        print(f'Evaluating {len(hyperparams)} hyperparameter configurations')\n",
    "        for i, hp in enumerate(hyperparams):\n",
    "            print(f'Evaluating HP {i+1}/{len(hyperparams)}')\n",
    "            self.HP = hp\n",
    "            self.train(debug=False, exp_id=i)\n",
    "            valid_loss, valid_perplexity = self.evaluate(self.valid_loader)\n",
    "            test_loss, test_perplexity = self.evaluate(self.test_loader)\n",
    "            hp_to_loss[hp] = (valid_loss, test_loss, valid_perplexity)\n",
    "        return hp_to_loss\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        hidden = self.init_hidden_layer(\n",
    "            self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "        )\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in data_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                # hidden = hidden.detach()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                total_samples += actual_batch_size\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        perplexity = math.exp(avg_loss) \n",
    "        return avg_loss, perplexity\n",
    "\n",
    "    def load_model(self, exp_dir: str):\n",
    "        model_weights_path = os.path.join(exp_dir, 'model_weights.pth')\n",
    "        hp_pickle_path = os.path.join(exp_dir, f'HP_{self.HP.__hash__()}.pkl')\n",
    "        if os.path.exists(hp_pickle_path):\n",
    "            with open(hp_pickle_path, 'rb') as f:\n",
    "                stored_hp = pickle.load(f)\n",
    "                if stored_hp == self.HP:\n",
    "                    print(\n",
    "                        f'Found existing model with the same hyperparameters: {self.HP}')\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def train(self, debug=True, exp_id: int = -1):\n",
    "        # Before training, if we've already trained a model\n",
    "        # with the exact same hyperparameters, we can load it instead of training\n",
    "        self.setup_training_data()\n",
    "        self.setup_training_model()\n",
    "        if exp_id == -1:\n",
    "            exp_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # Create experiment folder\n",
    "        experiment_folder = f'Experiment {exp_id}'\n",
    "        os.makedirs(experiment_folder, exist_ok=True)\n",
    "        if (self.load_model(experiment_folder)):\n",
    "            print(f'Skipping training for model: {self.HP}')\n",
    "            return\n",
    "        train_losses = []\n",
    "        train_perplexities = []\n",
    "        valid_losses = []\n",
    "        valid_perplexities = []\n",
    "        start_time = time.time()\n",
    "        for e in range(self.HP.num_epochs):\n",
    "            print(f\"Epoch {e+1}/{self.HP.num_epochs}\") if debug else None\n",
    "            # Initialize hidden layers on every epoch\n",
    "            hidden = self.init_hidden_layer(\n",
    "                self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "            )\n",
    "            epoch_loss = 0.0\n",
    "            for batch_idx, (x, y) in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                # Adjust the hidden state to match the actual batch size\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                print(\n",
    "                    f\"Processing batch {batch_idx + 1}/{len(self.train_loader)}\"\n",
    "                ) if debug else None\n",
    "                self.optimizer.zero_grad()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), max_norm=1)  # Clipping\n",
    "                self.optimizer.step()\n",
    "                # Prevent backprop through time?\n",
    "                hidden = hidden.detach()\n",
    "                epoch_loss += loss.item()\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{len(self.train_loader)}, Loss: {loss.item()}\"\n",
    "                ) if debug else None\n",
    "                # End of batch loop\n",
    "            avg_epoch_loss = epoch_loss / len(self.train_loader)\n",
    "            epoch_perplexity = math.exp(avg_epoch_loss)\n",
    "            train_perplexities.append(epoch_perplexity)\n",
    "            train_losses.append(avg_epoch_loss)\n",
    "\n",
    "            # Validation loss\n",
    "            valid_loss, valid_perplexity = self.evaluate(self.valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_perplexities.append(valid_perplexity)\n",
    "            print(\n",
    "                f\"Epoch {e+1}/{self.HP.num_epochs} Train Loss: {avg_epoch_loss}, Valid Loss: {valid_loss} Train Perplexity: {epoch_perplexity} Valid Perplexity: {valid_perplexity}\"\n",
    "            )\n",
    "            # End of epoch loop\n",
    "        end_time = time.time()\n",
    "        train_time_seconds = end_time - start_time\n",
    "        train_time_minutes = train_time_seconds / 60\n",
    "        train_time_hours = int(train_time_seconds // 3600)\n",
    "        train_time_minutes = int((train_time_seconds % 3600) // 60)\n",
    "        train_time_seconds = int(train_time_seconds % 60)\n",
    "        train_time_str = f'{train_time_hours:02d}:{train_time_minutes:02d}:{train_time_seconds:02d}'\n",
    "        print(\n",
    "            f'Training took {train_time_str} (HH:MM:SS)'\n",
    "        )\n",
    "\n",
    "        # Save the model weights and hyperparameters\n",
    "        torch.save(self.model.state_dict(), os.path.join(\n",
    "            experiment_folder, 'model_weights.pth'))\n",
    "\n",
    "        # Write hyperparameters to a pickle file\n",
    "        with open(os.path.join(experiment_folder, f'HP_{self.HP.__hash__()}.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.HP, f)\n",
    "\n",
    "        # Plot and save loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_losses,\n",
    "                 label='Training Loss', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_losses,\n",
    "                 label='Validation Loss', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder, f'loss_curve.png'))\n",
    "\n",
    "        # Plot and save perplexity\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_perplexities,\n",
    "                 label='Training Perplexity', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_perplexities,\n",
    "                 label='Validation Perplexity', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.title('Perplexity Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder,\n",
    "                    f'perplexity_curve.png'))\n",
    "\n",
    "    def setup_training_model(self):\n",
    "        print(f'Setting up training model with {self.HP}')\n",
    "        self.model = RNN(self.HP)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.HP.learning_rate)\n",
    "\n",
    "    def setup_training_data(self):\n",
    "        # Load the training data and create a vocabulary mapping\n",
    "        print(f'Loading data and creating vocabulary')\n",
    "        self.train_indices, self.train_vocab = self.load_data(self.train_file)\n",
    "        \n",
    "        # Create the vocabulary mapping from the training set\n",
    "        train_text = self.load_wikitext(self.train_file)\n",
    "        train_tokens = self.tokenize(train_text)\n",
    "        self.word_to_idx = self.create_vocab_mapping(train_tokens)\n",
    "        \n",
    "        # Use the same vocabulary mapping for validation and test sets\n",
    "        self.valid_indices, self.valid_vocab = self.load_data(self.valid_file, self.word_to_idx)\n",
    "        self.test_indices, self.test_vocab = self.load_data(self.test_file, self.word_to_idx)\n",
    "\n",
    "        print(f'Train vocab size: {self.train_vocab}')\n",
    "        print(f'Valid vocab size: {self.valid_vocab}')\n",
    "        print(f'Test vocab size: {self.test_vocab}')\n",
    "\n",
    "        print(f'Creating input-output pairs for the dataset')\n",
    "        # Create input-output pairs for the dataset\n",
    "        self.train_inputs, self.train_targets = self.create_sequences(\n",
    "            self.train_indices, self.HP.seq_length)\n",
    "        self.valid_inputs, self.valid_targets = self.create_sequences(\n",
    "            self.valid_indices, self.HP.seq_length)\n",
    "        self.test_inputs, self.test_targets = self.create_sequences(\n",
    "            self.test_indices, self.HP.seq_length)\n",
    "\n",
    "        print(f'Creating TensorDataset and DataLoader objects')\n",
    "        # Create the TensorDataset objects\n",
    "        self.train_dataset = TensorDataset(\n",
    "            self.train_inputs, self.train_targets)\n",
    "        self.valid_dataset = TensorDataset(\n",
    "            self.valid_inputs, self.valid_targets)\n",
    "        self.test_dataset = TensorDataset(self.test_inputs, self.test_targets)\n",
    "\n",
    "        # Create the DataLoader objects\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, self.HP.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset, self.HP.batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset, self.HP.batch_size, shuffle=False)\n",
    "\n",
    "    def init_hidden_layer(self, num_layers, batch_size, hidden_dim):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_dim).to(self.device)\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(0, len(data) - seq_length, seq_length):\n",
    "            inputs.append(data[i:i + seq_length])  # input sequence\n",
    "            # target sequence, shifted by one\n",
    "            targets.append(data[i + 1:i + seq_length + 1])\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def load_wikitext(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', ' <eol> ')\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split(' ')\n",
    "\n",
    "    def create_vocab_mapping(self, tokens, threshold=10):\n",
    "        counter = Counter(tokens)\n",
    "        sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top vocab_size tokens\n",
    "        sorted_tokens = sorted_tokens[:self.HP.vocab_size - 2]\n",
    "        # Create a mapping from words to indices\n",
    "        word_to_idx = {}\n",
    "        word_to_idx['<unk>'] = 0\n",
    "        word_to_idx['<eol>'] = 1\n",
    "        word_index = 2 # Start from 2 and assign indices to words as we travel through the sorted_tokens\n",
    "        for tkn, cnt in sorted_tokens:\n",
    "            if cnt >= threshold:\n",
    "                if tkn and tkn != '<eol>': # skip empty and <eol> tokens\n",
    "                    word_to_idx[tkn] = word_index\n",
    "                    word_index += 1\n",
    "        return word_to_idx\n",
    "\n",
    "    def tokens_to_indices(self, tokens, word_to_idx):\n",
    "        # return a list of indices (based on the dict mapping words to tokens)\n",
    "        return [word_to_idx.get(token, word_to_idx['<unk>']) for token in tokens if token]\n",
    "\n",
    "    def load_data(self, text, word_to_idx=None):\n",
    "        data_text = self.load_wikitext(text)\n",
    "        data_tokens = self.tokenize(data_text)\n",
    "        if word_to_idx is None:\n",
    "            # Create a new vocabulary mapping if none is provided\n",
    "            data_word_to_idx = self.create_vocab_mapping(data_tokens)\n",
    "        else:\n",
    "            # Use the provided vocabulary mapping\n",
    "            data_word_to_idx = word_to_idx\n",
    "        vocab_size = len(data_word_to_idx)\n",
    "        data_indices = self.tokens_to_indices(data_tokens, data_word_to_idx)\n",
    "        return data_indices, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Torch device: cuda\n",
      "Evaluating 2 hyperparameter configurations\n",
      "Evaluating HP 1/2\n",
      "Loading data and creating vocabulary\n",
      "Train vocab size: 9997\n",
      "Valid vocab size: 9997\n",
      "Test vocab size: 9997\n",
      "Creating input-output pairs for the dataset\n",
      "Creating TensorDataset and DataLoader objects\n",
      "Setting up training model with HP(vocab_size=10000, batch_size=64, seq_len=30, lr=0.0005, epochs=20, hl_dim=256, num_layers=1, emb_dim=100, do=0)\n",
      "Epoch 1/20 Train Loss: 5.717393961899421, Valid Loss: 0.08188436837902337 Train Perplexity: 304.111363582728 Valid Perplexity: 1.0853303040711169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# rnn_llm.train(debug=False)\u001b[39;00m\n\u001b[32m     30\u001b[39m hps = [hp, multi_layer_rnn]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m hp_to_loss_map = \u001b[43mrnn_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hp, (valid_loss, test_loss, valid_perplexity) \u001b[38;5;129;01min\u001b[39;00m hp_to_loss_map.items():\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m--------------------\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mRNNLLM.train_models\u001b[39m\u001b[34m(self, hyperparams)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEvaluating HP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hyperparams)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.HP = hp\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m valid_loss, valid_perplexity = \u001b[38;5;28mself\u001b[39m.evaluate(\u001b[38;5;28mself\u001b[39m.valid_loader)\n\u001b[32m     20\u001b[39m test_loss, test_perplexity = \u001b[38;5;28mself\u001b[39m.evaluate(\u001b[38;5;28mself\u001b[39m.test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mRNNLLM.train\u001b[39m\u001b[34m(self, debug, exp_id)\u001b[39m\n\u001b[32m    100\u001b[39m output, hidden = \u001b[38;5;28mself\u001b[39m.model(x, hidden)\n\u001b[32m    101\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_func(\n\u001b[32m    102\u001b[39m     output.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.HP.vocab_size), y.view(-\u001b[32m1\u001b[39m)\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m torch.nn.utils.clip_grad_norm_(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.parameters(), max_norm=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Clipping\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/autograd/__init__.py:340\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    331\u001b[39m inputs = (\n\u001b[32m    332\u001b[39m     (inputs,)\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    337\u001b[39m )\n\u001b[32m    339\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/autograd/__init__.py:196\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch.Tensor)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     out_numel_is_1 = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m == \u001b[32m1\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "hp = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=30,\n",
    "    learning_rate=0.0005,\n",
    "    num_epochs=20,\n",
    "    hidden_dim=256,\n",
    "    num_layers=1,\n",
    "    embedding_dim=100,\n",
    "    dropout=0\n",
    ")\n",
    "multi_layer_rnn = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=30,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=20,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    embedding_dim=100,\n",
    "    dropout=0.5\n",
    ")\n",
    "rnn_llm = RNNLLM(\n",
    "    train_valid_test_files=(\n",
    "        'wiki2.train.txt', 'wiki2.valid.txt', 'wiki2.test.txt'\n",
    "    ),\n",
    "    hp=hp\n",
    ")\n",
    "# rnn_llm.train(debug=False)\n",
    "hps = [hp, multi_layer_rnn]\n",
    "hp_to_loss_map = rnn_llm.train_models(hps)\n",
    "for hp, (valid_loss, test_loss, valid_perplexity) in hp_to_loss_map.items():\n",
    "    print('--------------------')\n",
    "    print(\n",
    "        f'{hp}\\nValidation Loss: {valid_loss}, Test Loss: {test_loss}, Validation Perplexity: {valid_perplexity}'\n",
    "    )\n",
    "    print('--------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
