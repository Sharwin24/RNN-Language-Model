{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: RNN Language Model\n",
    "\n",
    "## 1A. Description and Illustration of RNN Language Model architecture and design choices\n",
    "\n",
    "## 1B. Model Hyperparameters\n",
    "\n",
    "## 1C. Learning Curves\n",
    "\n",
    "## 1D. Final Test Set Perplexity\n",
    "\n",
    "## 2. Improving the RNN language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    '''Dataclass to store hyperparameters for the RNN Language Model'''\n",
    "\n",
    "    vocab_size: int\n",
    "    batch_size: int\n",
    "    seq_length: int\n",
    "    learning_rate: float\n",
    "    num_epochs: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    embedding_dim: int\n",
    "    dropout: float\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (\n",
    "                self.vocab_size,\n",
    "                self.batch_size,\n",
    "                self.seq_length,\n",
    "                self.learning_rate,\n",
    "                self.num_epochs,\n",
    "                self.hidden_dim,\n",
    "                self.num_layers,\n",
    "                self.embedding_dim,\n",
    "                self.dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __eq__(self, value: 'HyperParams'):\n",
    "        return (\n",
    "            self.vocab_size == value.vocab_size\n",
    "            and self.batch_size == value.batch_size\n",
    "            and self.seq_length == value.seq_length\n",
    "            and self.learning_rate == value.learning_rate\n",
    "            and self.num_epochs == value.num_epochs\n",
    "            and self.hidden_dim == value.hidden_dim\n",
    "            and self.num_layers == value.num_layers\n",
    "            and self.embedding_dim == value.embedding_dim\n",
    "            and self.dropout == value.dropout\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'HP(vocab_size={self.vocab_size}, batch_size={self.batch_size}, seq_len={self.seq_length}, lr={self.learning_rate}, epochs={self.num_epochs}, hl_dim={self.hidden_dim}, num_layers={self.num_layers}, emb_dim={self.embedding_dim}, do={self.dropout})'\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hp: HyperParams):\n",
    "        super().__init__()  # Remove 'self' from super() call\n",
    "        self.HP = hp\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(hp.vocab_size, hp.embedding_dim)\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=hp.embedding_dim,\n",
    "            hidden_size=hp.hidden_dim,\n",
    "            num_layers=hp.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=hp.dropout\n",
    "        )\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(hp.dropout)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hp.hidden_dim, hp.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass through RNN\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        # Take the last time step and pass through final layer\n",
    "        logits = self.dropout(rnn_out)\n",
    "        output = self.fc(logits)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN LLM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLLM:\n",
    "    def __init__(self, train_valid_test_files: tuple[str, str, str], hp: HyperParams):\n",
    "        self.train_file, self.valid_file, self.test_file = train_valid_test_files\n",
    "        self.HP = hp\n",
    "        os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "        os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using Torch device: {self.device}')\n",
    "\n",
    "    def train_models(self, hyperparams: list[HyperParams]):\n",
    "        hp_to_loss: dict[HyperParams, tuple[float, float, float]] = {}\n",
    "        # Each value in the dict is a tuple of (valid loss, test_loss, perplexity)\n",
    "        print(f'Evaluating {len(hyperparams)} hyperparameter configurations')\n",
    "        for i, hp in enumerate(hyperparams):\n",
    "            print(f'Evaluating HP {i+1}/{len(hyperparams)}')\n",
    "            self.HP = hp\n",
    "            self.train(debug=False, exp_id=i)\n",
    "            valid_loss, valid_perplexity = self.evaluate(self.valid_loader)\n",
    "            test_loss, test_perplexity = self.evaluate(self.test_loader)\n",
    "            hp_to_loss[hp] = (valid_loss, test_loss, valid_perplexity)\n",
    "        return hp_to_loss\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        hidden = self.init_hidden_layer(\n",
    "            self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "        )\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in data_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                hidden = self.init_hidden_layer(\n",
    "                    self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                )\n",
    "                # hidden = hidden.detach()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                total_samples += actual_batch_size\n",
    "        # total_samples = len(data_loader.dataset) #* self.HP.batch_size\n",
    "        avg_loss = total_loss / total_samples\n",
    "        perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf') # Prevent overflow\n",
    "        return avg_loss, perplexity\n",
    "\n",
    "    def load_model(self, exp_dir: str):\n",
    "        model_weights_path = os.path.join(exp_dir, 'model_weights.pth')\n",
    "        hp_pickle_path = os.path.join(exp_dir, f'HP_{self.HP.__hash__()}.pkl')\n",
    "        if os.path.exists(hp_pickle_path):\n",
    "            with open(hp_pickle_path, 'rb') as f:\n",
    "                stored_hp = pickle.load(f)\n",
    "                if stored_hp == self.HP:\n",
    "                    print(\n",
    "                        f'Found existing model with the same hyperparameters: {self.HP}')\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def train(self, debug=True, exp_id: int = -1):\n",
    "        # Before training, if we've already trained a model\n",
    "        # with the exact same hyperparameters, we can load it instead of training\n",
    "        self.setup_training_data()\n",
    "        self.setup_training_model()\n",
    "        if exp_id == -1:\n",
    "            exp_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # Create experiment folder\n",
    "        experiment_folder = f'Experiment {exp_id}'\n",
    "        os.makedirs(experiment_folder, exist_ok=True)\n",
    "        if (self.load_model(experiment_folder)):\n",
    "            print(f'Skipping training for model: {self.HP}')\n",
    "            return\n",
    "        train_losses = []\n",
    "        train_perplexities = []\n",
    "        valid_losses = []\n",
    "        valid_perplexities = []\n",
    "        start_time = time.time()\n",
    "        for e in range(self.HP.num_epochs):\n",
    "            print(f\"Epoch {e+1}/{self.HP.num_epochs}\") if debug else None\n",
    "            # Initialize hidden layers on every epoch\n",
    "            hidden = self.init_hidden_layer(\n",
    "                self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "            )\n",
    "            epoch_loss = 0.0\n",
    "            for batch_idx, (x, y) in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                # Adjust the hidden state to match the actual batch size\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                print(\n",
    "                    f\"Processing batch {batch_idx + 1}/{len(self.train_loader)}\"\n",
    "                ) if debug else None\n",
    "                self.optimizer.zero_grad()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), max_norm=1)  # Clipping\n",
    "                self.optimizer.step()\n",
    "                # Prevent backprop through time?\n",
    "                hidden = hidden.detach()\n",
    "                epoch_loss += loss.item()\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{len(self.train_loader)}, Loss: {loss.item()}\"\n",
    "                ) if debug else None\n",
    "                # End of batch loop\n",
    "            avg_epoch_loss = epoch_loss / len(self.train_loader)\n",
    "            epoch_perplexity = math.exp(avg_epoch_loss)\n",
    "            train_perplexities.append(epoch_perplexity)\n",
    "            train_losses.append(avg_epoch_loss)\n",
    "\n",
    "            # Validation loss\n",
    "            valid_loss, valid_perplexity = self.evaluate(self.valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_perplexities.append(valid_perplexity)\n",
    "            print(\n",
    "                f\"Epoch {e+1}/{self.HP.num_epochs} Train Loss: {avg_epoch_loss}, Valid Loss: {valid_loss} Train Perplexity: {epoch_perplexity} Valid Perplexity: {valid_perplexity}\"\n",
    "            )\n",
    "            # End of epoch loop\n",
    "        end_time = time.time()\n",
    "        train_time_seconds = end_time - start_time\n",
    "        train_time_minutes = train_time_seconds / 60\n",
    "        train_time_hours = int(train_time_seconds // 3600)\n",
    "        train_time_minutes = int((train_time_seconds % 3600) // 60)\n",
    "        train_time_seconds = int(train_time_seconds % 60)\n",
    "        train_time_str = f'{train_time_hours:02d}:{train_time_minutes:02d}:{train_time_seconds:02d}'\n",
    "        print(\n",
    "            f'Training took {train_time_str} (HH:MM:SS)'\n",
    "        )\n",
    "\n",
    "        # Save the model weights and hyperparameters\n",
    "        torch.save(self.model.state_dict(), os.path.join(\n",
    "            experiment_folder, 'model_weights.pth'))\n",
    "\n",
    "        # Write hyperparameters to a pickle file\n",
    "        with open(os.path.join(experiment_folder, f'HP_{self.HP.__hash__()}.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.HP, f)\n",
    "\n",
    "        # Plot and save loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_losses,\n",
    "                 label='Training Loss', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_losses,\n",
    "                 label='Validation Loss', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder, f'loss_curve.png'))\n",
    "\n",
    "        # Plot and save perplexity\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_perplexities,\n",
    "                 label='Training Perplexity', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_perplexities,\n",
    "                 label='Validation Perplexity', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.title('Perplexity Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder,\n",
    "                    f'perplexity_curve.png'))\n",
    "\n",
    "    def setup_training_model(self):\n",
    "        print(f'Setting up training model with {self.HP}')\n",
    "        self.model = RNN(self.HP)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.HP.learning_rate)\n",
    "\n",
    "    def setup_training_data(self):\n",
    "        # Load the data and create a reduced vocabulary\n",
    "        print(f'Loading data and creating reduced vocabulary')\n",
    "        self.train_indices, self.train_vocab = self.load_data(self.train_file)\n",
    "        self.valid_indices, self.valid_vocab = self.load_data(self.valid_file)\n",
    "        self.test_indices, self.test_vocab = self.load_data(self.test_file)\n",
    "\n",
    "        print(f'Train vocab size: {self.train_vocab}')\n",
    "        print(f'Valid vocab size: {self.valid_vocab}')\n",
    "        print(f'Test vocab size: {self.test_vocab}')\n",
    "\n",
    "        print(f'Creating input-output pairs for the dataset')\n",
    "        # Create input-output pairs for the dataset\n",
    "        self.train_inputs, self.train_targets = self.create_sequences(\n",
    "            self.train_indices, self.HP.seq_length)\n",
    "        self.valid_inputs, self.valid_targets = self.create_sequences(\n",
    "            self.valid_indices, self.HP.seq_length)\n",
    "        self.test_inputs, self.test_targets = self.create_sequences(\n",
    "            self.test_indices, self.HP.seq_length)\n",
    "\n",
    "        print(f'Creating TensorDataset and DataLoader objects')\n",
    "        # Create the TensorDataset objects\n",
    "        self.train_dataset = TensorDataset(\n",
    "            self.train_inputs, self.train_targets)\n",
    "        self.valid_dataset = TensorDataset(\n",
    "            self.valid_inputs, self.valid_targets)\n",
    "        self.test_dataset = TensorDataset(self.test_inputs, self.test_targets)\n",
    "\n",
    "        # Create the DataLoader objects\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, self.HP.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset, self.HP.batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset, self.HP.batch_size, shuffle=False)\n",
    "\n",
    "    def init_hidden_layer(self, num_layers, batch_size, hidden_dim):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_dim).to(self.device)\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(0, len(data) - seq_length, seq_length):\n",
    "            inputs.append(data[i:i + seq_length])  # input sequence\n",
    "            # target sequence, shifted by one\n",
    "            targets.append(data[i + 1:i + seq_length + 1])\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def load_wikitext(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', ' <eol> ')\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split(' ')\n",
    "\n",
    "    def create_vocab_mapping(self, tokens, threshold=10):\n",
    "        counter = Counter(tokens)\n",
    "        sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top vocab_size tokens\n",
    "        sorted_tokens = sorted_tokens[:self.HP.vocab_size - 2]\n",
    "        # Create a mapping from words to indices\n",
    "        word_to_idx = {}\n",
    "        word_to_idx['<unk>'] = 0\n",
    "        word_to_idx['<eol>'] = 1\n",
    "        word_index = 2 # Start from 2 and assign indices to words as we travel through the sorted_tokens\n",
    "        for tkn, cnt in sorted_tokens:\n",
    "            if cnt >= threshold:\n",
    "                if tkn and tkn != '<eol>': # skip empty and <eol> tokens\n",
    "                    word_to_idx[tkn] = word_index\n",
    "                    word_index += 1\n",
    "        return word_to_idx\n",
    "\n",
    "    def tokens_to_indices(self, tokens, word_to_idx):\n",
    "        # return a list of indices (based on the dict mapping words to tokens)\n",
    "        return [word_to_idx.get(token, word_to_idx['<unk>']) for token in tokens if token]\n",
    "\n",
    "    def load_data(self, text):\n",
    "        data_text = self.load_wikitext(text)\n",
    "        data_tokens = self.tokenize(data_text)\n",
    "        data_word_to_idx = self.create_vocab_mapping(data_tokens)\n",
    "        vocab_size = len(data_word_to_idx)\n",
    "        data_indices = self.tokens_to_indices(data_tokens, data_word_to_idx)\n",
    "        return data_indices, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Torch device: cuda\n",
      "Evaluating 2 hyperparameter configurations\n",
      "Evaluating HP 1/2\n",
      "Loading data and creating reduced vocabulary\n",
      "Train vocab size: 9997\n",
      "Valid vocab size: 2581\n",
      "Test vocab size: 2757\n",
      "Creating input-output pairs for the dataset\n",
      "Creating TensorDataset and DataLoader objects\n",
      "Setting up training model with HP(vocab_size=10000, batch_size=64, seq_len=30, lr=0.001, epochs=20, hl_dim=256, num_layers=1, emb_dim=100, do=0)\n",
      "Epoch 1/20 Train Loss: 5.510207974297159, Valid Loss: 0.1104629516338625 Train Perplexity: 247.20253349499015 Valid Perplexity: 1.1167949728566078\n",
      "Epoch 2/20 Train Loss: 4.934815100010703, Valid Loss: 0.1146747311975202 Train Perplexity: 139.047430286683 Valid Perplexity: 1.1215085864829206\n",
      "Epoch 3/20 Train Loss: 4.7186766691944175, Valid Loss: 0.11694434907175405 Train Perplexity: 112.0199151351063 Valid Perplexity: 1.1240568731414964\n",
      "Epoch 4/20 Train Loss: 4.579841001068845, Valid Loss: 0.11843808538038803 Train Perplexity: 97.49889075515162 Valid Perplexity: 1.1257371723554197\n"
     ]
    }
   ],
   "source": [
    "hp = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=30,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=20,\n",
    "    hidden_dim=256,\n",
    "    num_layers=1,\n",
    "    embedding_dim=100,\n",
    "    dropout=0\n",
    ")\n",
    "multi_layer_rnn = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=30,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=20,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    embedding_dim=100,\n",
    "    dropout=0.5\n",
    ")\n",
    "rnn_llm = RNNLLM(\n",
    "    train_valid_test_files=(\n",
    "        'wiki2.train.txt', 'wiki2.valid.txt', 'wiki2.test.txt'\n",
    "    ),\n",
    "    hp=hp\n",
    ")\n",
    "# rnn_llm.train(debug=False)\n",
    "hps = [hp, multi_layer_rnn]\n",
    "hp_to_loss_map = rnn_llm.train_models(hps)\n",
    "for hp, (valid_loss, test_loss, valid_perplexity) in hp_to_loss_map.items():\n",
    "    print('--------------------')\n",
    "    print(\n",
    "        f'{hp}\\nValidation Loss: {valid_loss}, Test Loss: {test_loss}, Validation Perplexity: {valid_perplexity}'\n",
    "    )\n",
    "    print('--------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
