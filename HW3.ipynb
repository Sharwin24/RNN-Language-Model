{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: RNN Language Model\n",
    "\n",
    "## 1A. Description and Illustration of RNN Language Model architecture and design choices\n",
    "\n",
    "## 1B. Model Hyperparameters\n",
    "\n",
    "## 1C. Learning Curves\n",
    "\n",
    "## 1D. Final Test Set Perplexity\n",
    "\n",
    "## 2. Improving the RNN language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    '''Dataclass to store hyperparameters for the RNN Language Model'''\n",
    "\n",
    "    vocab_size: int\n",
    "    batch_size: int\n",
    "    seq_length: int\n",
    "    learning_rate: float\n",
    "    num_epochs: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    embedding_dim: int\n",
    "    dropout: float\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (\n",
    "                self.vocab_size,\n",
    "                self.batch_size,\n",
    "                self.seq_length,\n",
    "                self.learning_rate,\n",
    "                self.num_epochs,\n",
    "                self.hidden_dim,\n",
    "                self.num_layers,\n",
    "                self.embedding_dim,\n",
    "                self.dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __eq__(self, value: 'HyperParams'):\n",
    "        return (\n",
    "            self.vocab_size == value.vocab_size\n",
    "            and self.batch_size == value.batch_size\n",
    "            and self.seq_length == value.seq_length\n",
    "            and self.learning_rate == value.learning_rate\n",
    "            and self.num_epochs == value.num_epochs\n",
    "            and self.hidden_dim == value.hidden_dim\n",
    "            and self.num_layers == value.num_layers\n",
    "            and self.embedding_dim == value.embedding_dim\n",
    "            and self.dropout == value.dropout\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'HP(vocab_size={self.vocab_size}, batch_size={self.batch_size}, seq_len={self.seq_length}, lr={self.learning_rate}, epochs={self.num_epochs}, hl_dim={self.hidden_dim}, num_layers={self.num_layers}, emb_dim={self.embedding_dim}, do={self.dropout})'\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hp: HyperParams):\n",
    "        super().__init__()  # Remove 'self' from super() call\n",
    "        self.HP = hp\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(hp.vocab_size, hp.embedding_dim)\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=hp.embedding_dim,\n",
    "            hidden_size=hp.hidden_dim,\n",
    "            num_layers=hp.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=hp.dropout\n",
    "        )\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(hp.dropout)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hp.hidden_dim, hp.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass through RNN\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        # Take the last time step and pass through final layer\n",
    "        logits = self.dropout(rnn_out)\n",
    "        output = self.fc(logits)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN LLM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLLM:\n",
    "    def __init__(self, train_valid_test_files: tuple[str, str, str], hp: HyperParams):\n",
    "        self.train_file, self.valid_file, self.test_file = train_valid_test_files\n",
    "        self.HP = hp\n",
    "        os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "        os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using Torch device: {self.device}')\n",
    "\n",
    "    def train_models(self, hyperparams: list[HyperParams]):\n",
    "        hp_to_loss: dict[HyperParams, tuple[float, float, float]] = {}\n",
    "        # Each value in the dict is a tuple of (valid loss, test_loss, perplexity)\n",
    "        print(f'Evaluating {len(hyperparams)} hyperparameter configurations')\n",
    "        for i, hp in enumerate(hyperparams):\n",
    "            print(f'Evaluating HP {i+1}/{len(hyperparams)}')\n",
    "            self.HP = hp\n",
    "            self.train(debug=False, exp_id=i)\n",
    "            valid_loss, valid_perplexity = self.evaluate(self.valid_loader)\n",
    "            test_loss, test_perplexity = self.evaluate(self.test_loader)\n",
    "            hp_to_loss[hp] = (valid_loss, test_loss, valid_perplexity)\n",
    "        return hp_to_loss\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        hidden = self.init_hidden_layer(\n",
    "            self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "        )\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in data_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                # hidden = hidden.detach()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1))\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        return avg_loss, perplexity\n",
    "\n",
    "    def load_model(self, exp_dir: str):\n",
    "        model_weights_path = os.path.join(exp_dir, 'model_weights.pth')\n",
    "        hp_pickle_path = os.path.join(exp_dir, f'HP_{self.HP.__hash__()}.pkl')\n",
    "        if os.path.exists(hp_pickle_path):\n",
    "            with open(hp_pickle_path, 'rb') as f:\n",
    "                stored_hp = pickle.load(f)\n",
    "                if stored_hp == self.HP:\n",
    "                    print(\n",
    "                        f'Found existing model with the same hyperparameters: {self.HP}')\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def train(self, debug=True, exp_id: int = -1):\n",
    "        # Before training, if we've already trained a model\n",
    "        # with the exact same hyperparameters, we can load it instead of training\n",
    "        self.setup_training_data()\n",
    "        self.setup_training_model()\n",
    "        if exp_id == -1:\n",
    "            exp_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # Create experiment folder\n",
    "        experiment_folder = f'Experiment {exp_id}'\n",
    "        os.makedirs(experiment_folder, exist_ok=True)\n",
    "        if (self.load_model(experiment_folder)):\n",
    "            print(f'Skipping training for model: {self.HP}')\n",
    "            return\n",
    "        train_losses = []\n",
    "        train_perplexities = []\n",
    "        valid_losses = []\n",
    "        valid_perplexities = []\n",
    "        start_time = time.time()\n",
    "        for e in range(self.HP.num_epochs):\n",
    "            print(f\"Epoch {e+1}/{self.HP.num_epochs}\") if debug else None\n",
    "            # Initialize hidden layers on every epoch\n",
    "            hidden = self.init_hidden_layer(\n",
    "                self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "            )\n",
    "            epoch_loss = 0.0\n",
    "            for batch_idx, (x, y) in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                # Adjust the hidden state to match the actual batch size\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                print(\n",
    "                    f\"Processing batch {batch_idx + 1}/{len(self.train_loader)}\"\n",
    "                ) if debug else None\n",
    "                self.optimizer.zero_grad()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), max_norm=5)  # Clipping\n",
    "                self.optimizer.step()\n",
    "                # Prevent backprop through time?\n",
    "                hidden = hidden.detach()\n",
    "                epoch_loss += loss.item()\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{len(self.train_loader)}, Loss: {loss.item()}\"\n",
    "                ) if debug else None\n",
    "                # End of batch loop\n",
    "            avg_epoch_loss = epoch_loss / len(self.train_loader)\n",
    "            epoch_perplexity = math.exp(avg_epoch_loss)\n",
    "            train_perplexities.append(epoch_perplexity)\n",
    "            train_losses.append(avg_epoch_loss)\n",
    "\n",
    "            # Validation loss\n",
    "            valid_loss, valid_perplexity = self.evaluate(self.valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_perplexities.append(valid_perplexity)\n",
    "            print(\n",
    "                f\"Epoch {e+1}/{self.HP.num_epochs} Train Loss: {avg_epoch_loss}, Valid Loss: {valid_loss} Train Perplexity: {epoch_perplexity} Valid Perplexity: {valid_perplexity}\"\n",
    "            )\n",
    "            # End of epoch loop\n",
    "        end_time = time.time()\n",
    "        train_time_seconds = end_time - start_time\n",
    "        train_time_minutes = train_time_seconds / 60\n",
    "        train_time_hours = int(train_time_seconds // 3600)\n",
    "        train_time_minutes = int((train_time_seconds % 3600) // 60)\n",
    "        train_time_seconds = int(train_time_seconds % 60)\n",
    "        train_time_str = f'{train_time_hours:02d}:{train_time_minutes:02d}:{train_time_seconds:02d}'\n",
    "        print(\n",
    "            f'Training took {train_time_str} (HH:MM:SS)'\n",
    "        )\n",
    "\n",
    "        # Save the model weights and hyperparameters\n",
    "        torch.save(self.model.state_dict(), os.path.join(\n",
    "            experiment_folder, 'model_weights.pth'))\n",
    "\n",
    "        # Write hyperparameters to a pickle file\n",
    "        with open(os.path.join(experiment_folder, f'HP_{self.HP.__hash__()}.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.HP, f)\n",
    "\n",
    "        # Plot and save loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_losses,\n",
    "                 label='Training Loss', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_losses,\n",
    "                 label='Validation Loss', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder, f'loss_curve.png'))\n",
    "\n",
    "        # Plot and save perplexity\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_perplexities,\n",
    "                 label='Training Perplexity', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_perplexities,\n",
    "                 label='Validation Perplexity', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.title('Perplexity Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder,\n",
    "                    f'perplexity_curve.png'))\n",
    "\n",
    "    def setup_training_model(self):\n",
    "        print(f'Setting up training model with {self.HP}')\n",
    "        self.model = RNN(self.HP)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.HP.learning_rate)\n",
    "\n",
    "    def setup_training_data(self):\n",
    "        # Load the training data and create a vocabulary mapping\n",
    "        print(f'Loading data and creating vocabulary')\n",
    "        self.train_indices, self.train_vocab = self.load_data(self.train_file)\n",
    "        \n",
    "        # Create the vocabulary mapping from the training set\n",
    "        train_text = self.load_wikitext(self.train_file)\n",
    "        train_tokens = self.tokenize(train_text)\n",
    "        self.word_to_idx = self.create_vocab_mapping(train_tokens)\n",
    "        \n",
    "        # Use the same vocabulary mapping for validation and test sets\n",
    "        self.valid_indices, self.valid_vocab = self.load_data(self.valid_file, self.word_to_idx)\n",
    "        self.test_indices, self.test_vocab = self.load_data(self.test_file, self.word_to_idx)\n",
    "\n",
    "        print(f'Train vocab size: {self.train_vocab}')\n",
    "        print(f'Valid vocab size: {self.valid_vocab}')\n",
    "        print(f'Test vocab size: {self.test_vocab}')\n",
    "\n",
    "        print(f'Creating input-output pairs for the dataset')\n",
    "        # Create input-output pairs for the dataset\n",
    "        self.train_inputs, self.train_targets = self.create_sequences(\n",
    "            self.train_indices, self.HP.seq_length)\n",
    "        self.valid_inputs, self.valid_targets = self.create_sequences(\n",
    "            self.valid_indices, self.HP.seq_length)\n",
    "        self.test_inputs, self.test_targets = self.create_sequences(\n",
    "            self.test_indices, self.HP.seq_length)\n",
    "\n",
    "        print(f'Creating TensorDataset and DataLoader objects')\n",
    "        # Create the TensorDataset objects\n",
    "        self.train_dataset = TensorDataset(\n",
    "            self.train_inputs, self.train_targets)\n",
    "        self.valid_dataset = TensorDataset(\n",
    "            self.valid_inputs, self.valid_targets)\n",
    "        self.test_dataset = TensorDataset(self.test_inputs, self.test_targets)\n",
    "\n",
    "        # Create the DataLoader objects\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, self.HP.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset, self.HP.batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset, self.HP.batch_size, shuffle=False)\n",
    "\n",
    "    def init_hidden_layer(self, num_layers, batch_size, hidden_dim):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_dim).to(self.device)\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(0, len(data) - seq_length, seq_length):\n",
    "            inputs.append(data[i:i + seq_length])  # input sequence\n",
    "            # target sequence, shifted by one\n",
    "            targets.append(data[i + 1:i + seq_length + 1])\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def load_wikitext(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().replace('\\n', ' <eol> ')\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split(' ')\n",
    "\n",
    "    def create_vocab_mapping(self, tokens, threshold=10):\n",
    "        counter = Counter(tokens)\n",
    "        sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top vocab_size tokens\n",
    "        sorted_tokens = sorted_tokens[:self.HP.vocab_size - 2]\n",
    "        # Create a mapping from words to indices\n",
    "        word_to_idx = {}\n",
    "        word_to_idx['<unk>'] = 0\n",
    "        word_to_idx['<eol>'] = 1\n",
    "        word_index = 2 # Start from 2 and assign indices to words as we travel through the sorted_tokens\n",
    "        for tkn, cnt in sorted_tokens:\n",
    "            if cnt >= threshold:\n",
    "                if tkn and tkn != '<eol>': # skip empty and <eol> tokens\n",
    "                    word_to_idx[tkn] = word_index\n",
    "                    word_index += 1\n",
    "        return word_to_idx\n",
    "\n",
    "    def tokens_to_indices(self, tokens, word_to_idx):\n",
    "        # return a list of indices (based on the dict mapping words to tokens)\n",
    "        return [word_to_idx.get(token, word_to_idx['<unk>']) for token in tokens if token]\n",
    "\n",
    "    def load_data(self, text, word_to_idx=None):\n",
    "        data_text = self.load_wikitext(text)\n",
    "        data_tokens = self.tokenize(data_text)\n",
    "        if word_to_idx is None:\n",
    "            # Create a new vocabulary mapping if none is provided\n",
    "            data_word_to_idx = self.create_vocab_mapping(data_tokens)\n",
    "        else:\n",
    "            # Use the provided vocabulary mapping\n",
    "            data_word_to_idx = word_to_idx\n",
    "        vocab_size = len(data_word_to_idx)\n",
    "        data_indices = self.tokens_to_indices(data_tokens, data_word_to_idx)\n",
    "        return data_indices, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Torch device: cuda\n",
      "Evaluating 2 hyperparameter configurations\n",
      "Evaluating HP 1/2\n",
      "Loading data and creating vocabulary\n",
      "Train vocab size: 9997\n",
      "Valid vocab size: 9997\n",
      "Test vocab size: 9997\n",
      "Creating input-output pairs for the dataset\n",
      "Creating TensorDataset and DataLoader objects\n",
      "Setting up training model with HP(vocab_size=10000, batch_size=64, seq_len=30, lr=0.001, epochs=20, hl_dim=256, num_layers=1, emb_dim=100, do=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# rnn_llm.train(debug=False)\u001b[39;00m\n\u001b[32m     30\u001b[39m hps = [hp, multi_layer_rnn]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m hp_to_loss_map = \u001b[43mrnn_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hp, (valid_loss, test_loss, valid_perplexity) \u001b[38;5;129;01min\u001b[39;00m hp_to_loss_map.items():\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m--------------------\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mRNNLLM.train_models\u001b[39m\u001b[34m(self, hyperparams)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEvaluating HP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hyperparams)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.HP = hp\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m valid_loss, valid_perplexity = \u001b[38;5;28mself\u001b[39m.evaluate(\u001b[38;5;28mself\u001b[39m.valid_loader)\n\u001b[32m     20\u001b[39m test_loss, test_perplexity = \u001b[38;5;28mself\u001b[39m.evaluate(\u001b[38;5;28mself\u001b[39m.test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mRNNLLM.train\u001b[39m\u001b[34m(self, debug, exp_id)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     94\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m ) \u001b[38;5;28;01mif\u001b[39;00m debug \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_func(\n\u001b[32m     99\u001b[39m     output.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.HP.vocab_size), y.view(-\u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m )\n\u001b[32m    101\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mRNN.forward\u001b[39m\u001b[34m(self, x, hidden)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Take the last time step and pass through final layer\u001b[39;00m\n\u001b[32m     72\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.dropout(rnn_out)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Winter2025/DeepLearning/dl_venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "hp = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=30,\n",
    "    learning_rate=0.0005,\n",
    "    num_epochs=20,\n",
    "    hidden_dim=256,\n",
    "    num_layers=1,\n",
    "    embedding_dim=100,\n",
    "    dropout=0\n",
    ")\n",
    "multi_layer_rnn = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=30,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=20,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    embedding_dim=100,\n",
    "    dropout=0.5\n",
    ")\n",
    "rnn_llm = RNNLLM(\n",
    "    train_valid_test_files=(\n",
    "        'wiki2.train.txt', 'wiki2.valid.txt', 'wiki2.test.txt'\n",
    "    ),\n",
    "    hp=hp\n",
    ")\n",
    "# rnn_llm.train(debug=False)\n",
    "hps = [hp, multi_layer_rnn]\n",
    "hp_to_loss_map = rnn_llm.train_models(hps)\n",
    "for hp, (valid_loss, test_loss, valid_perplexity) in hp_to_loss_map.items():\n",
    "    print('--------------------')\n",
    "    print(\n",
    "        f'{hp}\\nValidation Loss: {valid_loss}, Test Loss: {test_loss}, Validation Perplexity: {valid_perplexity}'\n",
    "    )\n",
    "    print('--------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
