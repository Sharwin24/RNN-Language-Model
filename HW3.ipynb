{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: RNN Language Model\n",
    "\n",
    "## 1A. Description and Illustration of RNN Language Model architecture and design choices\n",
    "\n",
    "## 1B. Model Hyperparameters\n",
    "\n",
    "## 1C. Learning Curves\n",
    "\n",
    "## 1D. Final Test Set Perplexity\n",
    "\n",
    "## 2. Improving the RNN language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    '''Dataclass to store hyperparameters for the RNN Language Model'''\n",
    "\n",
    "    vocab_size: int\n",
    "    batch_size: int\n",
    "    seq_length: int\n",
    "    learning_rate: float\n",
    "    num_epochs: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    embedding_dim: int\n",
    "    dropout: float\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (\n",
    "                self.vocab_size,\n",
    "                self.batch_size,\n",
    "                self.seq_length,\n",
    "                self.learning_rate,\n",
    "                self.num_epochs,\n",
    "                self.hidden_dim,\n",
    "                self.num_layers,\n",
    "                self.embedding_dim,\n",
    "                self.dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'HP(vocab_size={self.vocab_size}, batch_size={self.batch_size}, seq_len={self.seq_length}, lr={self.learning_rate}, epochs={self.num_epochs}, hl_dim={self.hidden_dim}, num_layers={self.num_layers}, emb_dim={self.embedding_dim}, do={self.dropout})'\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hp: HyperParams):\n",
    "        super().__init__()  # Remove 'self' from super() call\n",
    "        self.HP = hp\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(hp.vocab_size, hp.embedding_dim)\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=hp.embedding_dim,\n",
    "            hidden_size=hp.hidden_dim,\n",
    "            num_layers=hp.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=hp.dropout\n",
    "        )\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hp.hidden_dim, hp.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass through RNN\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)\n",
    "        # Take the last time step and pass through final layer\n",
    "        output = self.fc(rnn_out)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN LLM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLLM:\n",
    "    def __init__(self, train_valid_test_files: tuple[str, str, str], hp: HyperParams):\n",
    "        self.train_file, self.valid_file, self.test_file = train_valid_test_files\n",
    "        self.HP = hp\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using Torch device: {self.device}')\n",
    "\n",
    "    def find_best_hyperparams(self, hyperparams: list[HyperParams]):\n",
    "        hp_to_loss: dict[HyperParams, tuple[float, float, float]] = {}\n",
    "        # Each value in the dict is a tuple of (valid loss, test_loss, perplexity)\n",
    "        print(f'Evaluating {len(hyperparams)} hyperparameter configurations')\n",
    "        for i, hp in enumerate(hyperparams):\n",
    "            print(f'Evaluating HP {i+1}/{len(hyperparams)}')\n",
    "            self.HP = hp\n",
    "            self.train(debug=False, exp_id=i)\n",
    "            valid_loss = self.evaluate(self.valid_loader)\n",
    "            test_loss = self.evaluate(self.test_loader)\n",
    "            perplexity = torch.exp(torch.tensor(valid_loss)).item()\n",
    "            hp_to_loss[hp] = (valid_loss, test_loss, perplexity)\n",
    "        # Return the hyperparameters with the lowest validation loss\n",
    "        # return min(hp_to_loss, key=hp_to_loss.get)\n",
    "        return hp_to_loss\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        hidden = self.init_hidden_layer(\n",
    "            self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "        )\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in data_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1))\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        return avg_loss\n",
    "\n",
    "    def load_model(self, exp_dir: str):\n",
    "        model_weights_path = os.path.join(exp_dir, 'model_weights.pth')\n",
    "        if os.path.exists(model_weights_path):\n",
    "            self.model.load_state_dict(torch.load(model_weights_path))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def train(self, debug=True, exp_id: int = -1):\n",
    "        # Before training, if we've already trained a model\n",
    "        # with the exact same hyperparameters, we can load it instead of training\n",
    "        self.setup_training_data()\n",
    "        self.setup_training_model()\n",
    "        if exp_id == -1:\n",
    "            exp_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # Create experiment folder\n",
    "        experiment_folder = f'Experiment {exp_id}'\n",
    "        os.makedirs(experiment_folder, exist_ok=True)\n",
    "        if (self.load_model(experiment_folder)):\n",
    "            print(f'Skipping training for model: {self.HP}')\n",
    "            return\n",
    "        train_losses = []\n",
    "        train_perplexity = []\n",
    "        valid_losses = []\n",
    "        valid_perplexity = []\n",
    "        start_time = time.time()\n",
    "        for e in range(self.HP.num_epochs):\n",
    "            print(f\"Epoch {e+1}/{self.HP.num_epochs}\") if debug else None\n",
    "            # Initialize hidden layers on every epoch\n",
    "            hidden = self.init_hidden_layer(\n",
    "                self.HP.num_layers, self.HP.batch_size, self.HP.hidden_dim\n",
    "            )\n",
    "            epoch_loss = 0.0\n",
    "            for batch_idx, (x, y) in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                actual_batch_size = x.size(0)\n",
    "                # Adjust the hidden state to match the actual batch size\n",
    "                if hidden.size(1) != actual_batch_size:\n",
    "                    hidden = self.init_hidden_layer(\n",
    "                        self.HP.num_layers, actual_batch_size, self.HP.hidden_dim\n",
    "                    )\n",
    "                print(\n",
    "                    f\"Processing batch {batch_idx + 1}/{len(self.train_loader)}\"\n",
    "                ) if debug else None\n",
    "                self.optimizer.zero_grad()\n",
    "                output, hidden = self.model(x, hidden)\n",
    "                loss = self.loss_func(\n",
    "                    output.view(-1, self.HP.vocab_size), y.view(-1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # Prevent backprop through time?\n",
    "                hidden = hidden.detach()\n",
    "                epoch_loss += loss.item()\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1}/{len(self.train_loader)}, Loss: {loss.item()}\"\n",
    "                ) if debug else None\n",
    "                # End of batch loop\n",
    "            avg_epoch_loss = epoch_loss / len(self.train_loader)\n",
    "            epoch_perplexity = torch.exp(torch.tensor(avg_epoch_loss)).item()\n",
    "            train_perplexity.append(epoch_perplexity)\n",
    "            train_losses.append(avg_epoch_loss)\n",
    "\n",
    "            # Validation loss\n",
    "            valid_loss = self.evaluate(self.valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_perplexity.append(torch.exp(torch.tensor(valid_loss)).item())\n",
    "            print(\n",
    "                f\"Epoch {e+1}/{self.HP.num_epochs} Train Loss: {avg_epoch_loss}, Valid Loss: {valid_loss} Perplexity: {epoch_perplexity}\"\n",
    "            )\n",
    "            # End of epoch loop\n",
    "        end_time = time.time()\n",
    "        train_time_seconds = end_time - start_time\n",
    "        train_time_minutes = train_time_seconds / 60\n",
    "        train_time_hours = int(train_time_seconds // 3600)\n",
    "        train_time_minutes = int((train_time_seconds % 3600) // 60)\n",
    "        train_time_seconds = int(train_time_seconds % 60)\n",
    "        train_time_str = f'{train_time_hours:02d}:{train_time_minutes:02d}:{train_time_seconds:02d}'\n",
    "        print(\n",
    "            f'Training took {train_time_str} (HH:MM:SS)'\n",
    "        )\n",
    "\n",
    "        # Save the model weights and hyperparameters\n",
    "        torch.save(self.model.state_dict(), os.path.join(\n",
    "            experiment_folder, 'model_weights.pth'))\n",
    "\n",
    "        # Write hyperparameters to a pickle file\n",
    "        with open(os.path.join(experiment_folder, 'hyperparameters.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.HP, f)\n",
    "\n",
    "        # Plot and save loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_losses,\n",
    "                 label='Training Loss', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_losses,\n",
    "                 label='Validation Loss', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder, f'loss_curve.png'))\n",
    "\n",
    "        # Plot and save perplexity\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), train_perplexity,\n",
    "                 label='Training Perplexity', marker='o', color='b')\n",
    "        plt.plot(range(1, self.HP.num_epochs + 1), valid_perplexity,\n",
    "                 label='Validation Perplexity', marker='o', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.title('Perplexity Curve')\n",
    "        plt.legend()\n",
    "        plt.figtext(\n",
    "            0.15, 0.85, f'Training Time: {train_time_str} (HH::MM::SS)', fontsize=10, ha='left'\n",
    "        )\n",
    "        plt.savefig(os.path.join(experiment_folder,\n",
    "                    f'perplexity_curve.png'))\n",
    "\n",
    "    def setup_training_model(self):\n",
    "        print(f'Setting up training model with {self.HP}')\n",
    "        self.model = RNN(self.HP)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.HP.learning_rate)\n",
    "\n",
    "    def setup_training_data(self):\n",
    "        # Load the data and create a reduced vocabulary\n",
    "        print(f'Loading data and creating reduced vocabulary')\n",
    "        self.train_indices, self.train_vocab = self.load_data(self.train_file)\n",
    "        self.valid_indices, self.valid_vocab = self.load_data(self.valid_file)\n",
    "        self.test_indices, self.test_vocab = self.load_data(self.test_file)\n",
    "\n",
    "        print(f'Train vocab size: {self.train_vocab}')\n",
    "        print(f'Valid vocab size: {self.valid_vocab}')\n",
    "        print(f'Test vocab size: {self.test_vocab}')\n",
    "\n",
    "        print(f'Creating input-output pairs for the dataset')\n",
    "        # Create input-output pairs for the dataset\n",
    "        self.train_inputs, self.train_targets = self.create_sequences(\n",
    "            self.train_indices, self.HP.seq_length)\n",
    "        self.valid_inputs, self.valid_targets = self.create_sequences(\n",
    "            self.valid_indices, self.HP.seq_length)\n",
    "        self.test_inputs, self.test_targets = self.create_sequences(\n",
    "            self.test_indices, self.HP.seq_length)\n",
    "\n",
    "        print(f'Creating TensorDataset and DataLoader objects')\n",
    "        # Create the TensorDataset objects\n",
    "        self.train_dataset = TensorDataset(\n",
    "            self.train_inputs, self.train_targets)\n",
    "        self.valid_dataset = TensorDataset(\n",
    "            self.valid_inputs, self.valid_targets)\n",
    "        self.test_dataset = TensorDataset(self.test_inputs, self.test_targets)\n",
    "\n",
    "        # Create the DataLoader objects\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, self.HP.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset, self.HP.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset, self.HP.batch_size, shuffle=True)\n",
    "\n",
    "    def init_hidden_layer(self, num_layers, batch_size, hidden_dim):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_dim).to(self.device)\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            inputs.append(data[i:i + seq_length])  # input sequence\n",
    "            # target sequence, shifted by one\n",
    "            targets.append(data[i + 1:i + seq_length + 1])\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def load_wikitext(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split(' ')\n",
    "\n",
    "    def create_vocab_mapping(self, tokens):\n",
    "        vocab = sorted(set(tokens))[:self.HP.vocab_size - 1]\n",
    "        vocab.append('<unk>')\n",
    "        word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        return word_to_idx\n",
    "\n",
    "    def tokens_to_indices(self, tokens, word_to_idx):\n",
    "        # return a list of indices (based on the dict mapping words to tokens)\n",
    "        return [word_to_idx.get(token, word_to_idx['<unk>']) for token in tokens]\n",
    "\n",
    "    def load_data(self, text):\n",
    "        data_text = self.load_wikitext(text)\n",
    "        data_tokens = self.tokenize(data_text)\n",
    "        # reduced_data_tokens = self.reduce_vocab(data_tokens)\n",
    "        data_word_to_idx = self.create_vocab_mapping(data_tokens)\n",
    "        vocab_size = len(data_word_to_idx)\n",
    "        data_indices = self.tokens_to_indices(data_tokens, data_word_to_idx)\n",
    "        return data_indices, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=20,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=2,\n",
    "    hidden_dim=256,\n",
    "    num_layers=1,\n",
    "    embedding_dim=100,\n",
    "    dropout=0\n",
    ")\n",
    "multi_layer_rnn = HyperParams(\n",
    "    vocab_size=10000,\n",
    "    batch_size=64,\n",
    "    seq_length=20,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=2,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    embedding_dim=100,\n",
    "    dropout=0.3\n",
    ")\n",
    "rnn_llm = RNNLLM(\n",
    "    train_valid_test_files=(\n",
    "        'wiki2.train.txt', 'wiki2.valid.txt', 'wiki2.test.txt'\n",
    "    ),\n",
    "    hp=hp\n",
    ")\n",
    "# rnn_llm.train(debug=False)\n",
    "hps = [hp, multi_layer_rnn]\n",
    "hp_to_loss_map = rnn_llm.find_best_hyperparams(hps)\n",
    "for hp, (valid_loss, test_loss, perplexity) in hp_to_loss_map.items():\n",
    "    print('--------------------')\n",
    "    print(\n",
    "        f'{hp}\\nValidation Loss: {valid_loss}, Test Loss: {test_loss}, Perplexity: {perplexity}'\n",
    "    )\n",
    "    print('--------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
